#!/usr/bin/python
# -*- coding:utf-8 -*-

import logging
import os
import time
import warnings
import random

import torch
from torch import nn
from torch import optim
import numpy as np
import matplotlib.pyplot as plt
from scipy.signal import resample

from sklearn.metrics import precision_score, recall_score, f1_score, classification_report
from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

import torch.nn.functional as F

import datasets as datasets
import models.LORA_Net_12345 as models
#import models.LORA_Net_12345 as model_Ftest
#import models.wd as model_wd


import os  
os.environ['CUDA_LAUNCH_BLOCKING'] = "1"

from collections import OrderedDict


from sklearn.preprocessing import MultiLabelBinarizer

# def set_seed(seed=99):
#     random.seed(seed)  # è®¾ç½® Python éšæœºç§å­
#     np.random.seed(seed)  # è®¾ç½® NumPy éšæœºç§å­
#     torch.manual_seed(seed)  # è®¾ç½® PyTorch CPU çš„éšæœºç§å­
#     torch.cuda.manual_seed(seed)  # å¦‚æœä½¿ç”¨ GPUï¼Œè®¾ç½® GPU çš„éšæœºç§å­
#     torch.cuda.manual_seed_all(seed)  # å¦‚æœä½¿ç”¨å¤šä¸ª GPUï¼Œè®¾ç½®æ‰€æœ‰ GPU çš„éšæœºç§å­

#     # ä¿è¯ç»“æœçš„å¯å¤ç°æ€§
#     torch.backends.cudnn.deterministic = True  # ä¿è¯æ¯æ¬¡è¿”å›çš„å·ç§¯ç®—æ³•æ˜¯ç¡®å®šçš„
#     torch.backends.cudnn.benchmark = False     # å¦‚æœä¸º Trueï¼Œç³»ç»Ÿä¼šè‡ªåŠ¨å¯»æ‰¾æœ€é€‚åˆå½“å‰é…ç½®çš„é«˜æ•ˆç®—æ³•ï¼Œä½†ä¼šå¼•å…¥éšæœºæ€§

# # ä½¿ç”¨ç‰¹å®šçš„éšæœºç§å­
# set_seed(99)

 # ** è®°å¿†æ± å¤§å° **
MEMORY_SIZE = 2000

def apply_dropout(m):
    if type(m) == nn.Dropout:
        m.eval()
class train_utils(object):
    def __init__(self, args, save_dir):
        self.args = args
        self.save_dir = save_dir
        # self.loss_values = []

        # self.mlb = MultiLabelBinarizer(classes=np.arange(args.class_num))
        self.train_dict = OrderedDict()
        self.train_dict['source_train-Loss'] = []
        self.train_dict['source_train-Acc_pos'] = []
  
        self.train_dict['source_val-Loss'] = []
        self.train_dict['source_val-Acc_pos'] = []
        
        self.train_dict['target_val-Loss'] = []
        self.train_dict['target_val-Acc_pos'] = []   

        self.Fine_dict = OrderedDict() 
        self.Fine_dict['Fine_Acc'] = []  
       
    def setup(self):
        """
        Initialize the datasets, model, loss and optimizer
        :param args:
        :return:
        """
        args = self.args

        # Consider the gpu or cpu condition
        if torch.cuda.is_available():
            self.device = torch.device("cuda")
            self.device_count = torch.cuda.device_count()
            logging.info('using {} gpus'.format(self.device_count))
            assert args.batch_size % self.device_count == 0, "batch size should be divided by device count"
        else:
            warnings.warn("gpu is not available")
            self.device = torch.device("cpu")
            self.device_count = 1
            logging.info('using {} cpu'.format(self.device_count))

        # Load the datasets
        Dataset = getattr(datasets,args.data_name)
        self.datasets = {}


        if isinstance(args.transfer_task[0], str):
           #print( args.transfer_task)
           args.transfer_task = eval("".join(args.transfer_task))


        self.datasets['source_train'], self.datasets['source_val'], self.datasets['target_val'] = Dataset(args.data_dir, args.transfer_task, args.normlizetype, args.source_num_classes).data_split(transfer_learning=False)
        # åŠ è½½æ‰€æœ‰ç›®æ ‡åŸŸçš„æ•°æ®é›†
        # self.data_test = Dataset(args.data_dir, args.transfer_task, args.normlizetype).data_test()     

        self.dataloaders = {x: torch.utils.data.DataLoader(self.datasets[x], batch_size=args.batch_size, drop_last=True,
                                                           shuffle=(True if x.split('_')[1] == 'train' else False),
                                                           num_workers=args.num_workers,
                                                           pin_memory=(True if self.device == 'cuda' else False),
                                                           #worker_init_fn=lambda worker_id: np.random.seed(99)
                                                           )
                            for x in ['source_train', 'source_val', 'target_val']}
        
        # Define the model
        self.model = getattr(models,args.model_name)(args.pretrained)
#        self.model_Ftest = getattr(model_Ftest, args.model_Fine_name)(args.pretrained)


        
        self.model_test = getattr(models, args.model_name)(args.pretrained)

        
        
        # self.model.fc = torch.nn.Linear(self.model.fc.in_features, Dataset.num_classes)
    
        if args.adabn:
            self.model_eval = getattr(models, args.model_name)(args.pretrained)
        
        if self.device_count > 1:
            self.model = torch.nn.DataParallel(self.model)
            if args.adabn:
                self.model_eval = torch.nn.DataParallel(self.model_eval)

        # Define the optimizer
        if args.opt == 'adam':
            self.optimizer = optim.Adam(filter(lambda p: p.requires_grad, self.model.parameters(self)), lr=args.lr,
                                        weight_decay=args.weight_decay)
        elif args.opt == 'sgd':
            self.optimizer = optim.SGD(filter(lambda p: p.requires_grad, self.model.parameters()), lr=args.lr,
                                       momentum=args.momentum, weight_decay=args.weight_decay)
        else:
            raise Exception("optimizer not implement")


        # Define the learning rate decay
        if args.lr_scheduler == 'step':
            steps = [int(step) for step in args.steps.split(',')]
            self.lr_scheduler = optim.lr_scheduler.MultiStepLR(self.optimizer, steps, gamma=args.gamma)
        elif args.lr_scheduler == 'exp':
            self.lr_scheduler = optim.lr_scheduler.ExponentialLR(self.optimizer, args.gamma)
        elif args.lr_scheduler == 'stepLR':
            steps = int(args.steps)
            self.lr_scheduler = optim.lr_scheduler.StepLR(self.optimizer, steps, args.gamma)
        elif args.lr_scheduler == 'cos':
            self.lr_scheduler = optim.lr_scheduler.CosineAnnealingLR(self.optimizer, 150, 1e-4)
        elif args.lr_scheduler == 'fix':
            self.lr_scheduler = None
        else:
            raise Exception("lr schedule not implement")

        self.start_epoch = 0


        # Invert the model and define the loss
        self.model.to(self.device)
        if args.adabn:
            self.model_eval.to(self.device)

        self.criterion = nn.CrossEntropyLoss()
        # self.criterion = nn.SoftmaxCrossEntropyLoss()
    

    def set_input(self, input):
        """input: a dictionary that contains the data itself and its metadata information.
        example:
            input_signals.size() = B, C, 1, S

        """
        data_set = []
        for data in input:
            shift = np.random.randint(-100, 400)
            # éšæœºç”Ÿæˆä¸€ä¸ªåç§»é‡shift
            scale = np.random.uniform(0.5, 1.5)
            # éšæœºç”Ÿæˆä¸€ä¸ªç¼©æ”¾å› å­scale
            
            data_ = np.roll( data , shift ) * scale
            
            # transformed_signal1 = np.roll(data[:, 0], shift) * scale
            # transformed_signal2 = np.roll(data[:, 1], shift) * scale
            # transformed_signal3 = np.roll(data[:, 2], shift) * scale
            # # åˆ†åˆ«å¯¹dataæ•°æ®é›†çš„ç¬¬ä¸€ã€ç¬¬äºŒã€ç¬¬ä¸‰åˆ—è¿›è¡Œå¤„ç†
                   
            # data_ = np.concatenate([transformed_signal1.reshape(-1, 1), transformed_signal2.reshape(-1, 1),
            #                         transformed_signal3.reshape(-1, 1)], axis=1)
            # data_ = np.concatenate([transformed_signal1.reshape(-1, 1), transformed_signal2.reshape(-1, 1),
            #                         transformed_signal3.reshape(-1, 1)], axis=1)
            # data_ = np.concatenate([transformed_signal1.reshape(-1, 1),
            #                         transformed_signal2.reshape(-1, 1)], axis=1)
            # å˜ä¸ºå¤šè¡Œä¸€åˆ—çš„å½¢å¼ï¼Œæ²¿xè½´ä½œä¸ºä¸€ç»„

            data_set.append(data_)
            self.input_signals = torch.tensor(data_set)
            
        # self.input_signals = torch.from_numpy(np.transpose(np.array(data_set), (0, 2, 1))).float()#.to(self.device)
        # np.transposeæ”¹å˜æ•°ç»„çš„ç»´åº¦ä½ç½®;(0,1,2)->(0,2,1)

        # self.pos = (torch.tensor(labels).float() / self.args.dot_num).reshape(-1, 1)


        # self.cla = torch.from_numpy(self.mlb.fit_transform(labels)).float()#.to(self.args.device) #ç±»åˆ«ä»»åŠ¡
        # ä½¿ç”¨ mlb.fit_transform æ–¹æ³•å°†ç›®æ ‡æ•°æ®ä¸­çš„ç¬¬äºŒåˆ—è¿›è¡Œ one-hot ç¼–ç ,å¹¶è½¬æ¢æ ¼å¼
        ############################################################################################################################

        return self.input_signals
    
    # def wd(self,input):

    #   self.wd = getattr(model_wd,'WaveletGatedNet')(signal_length=1792, wavelet_name='db1',level=8)  
      
    #   return self.wd(input) 
    

   
    memory_buffer = {"x": [], "y": [], "acc": []}  # å­˜å‚¨ä¿¡å·æ•°æ®ã€æ ‡ç­¾ã€å¯¹åº”å‡†ç¡®ç‡

    
    # è®°å¿†æ± æ›´æ–°å‡½æ•°ï¼ŒåŸºäºå‡†ç¡®ç‡ä¼˜åŒ–è®°å¿†æ± ï¼Œæ›¿æ¢ä½è´¨é‡æ ·æœ¬
    def update_memory(x_batch, y_batch, batch_accuracy):
        # å®šä¹‰å…¨å±€å˜é‡ è®°å¿†æ± 
        global memory_buffer

        x_batch = x_batch.cpu().numpy().tolist()
        y_batch = y_batch.cpu().numpy().tolist()

        # è®¡ç®—è®°å¿†æ± å¹³å‡å‡†ç¡®ç‡
        memory_avg_acc = np.mean(memory_buffer["acc"]) if memory_buffer["acc"] else 0

        if batch_accuracy > memory_avg_acc:
            print(f"ğŸ”„ å½“å‰ batch å‡†ç¡®ç‡ {batch_accuracy:.2f} > è®°å¿†æ± å¹³å‡ {memory_avg_acc:.2f}ï¼Œè¿›è¡Œä¼˜åŒ–")

            if len(memory_buffer["x"]) < MEMORY_SIZE:
                # è®°å¿†æ± æœªæ»¡ï¼Œç›´æ¥åŠ å…¥
                memory_buffer["x"].extend(x_batch)
                memory_buffer["y"].extend(y_batch)
                memory_buffer["acc"].extend([batch_accuracy] * len(x_batch))
            else:
                # è®°å¿†æ± å·²æ»¡ï¼Œæ‰¾åˆ°å‡†ç¡®ç‡æœ€ä½çš„æ ·æœ¬è¿›è¡Œæ›¿æ¢
                sorted_indices = np.argsort(memory_buffer["acc"])  # æŒ‰å‡†ç¡®ç‡å‡åºæ’åº
                worst_indices = sorted_indices[:len(x_batch)]  # é€‰æ‹©æœ€å·®çš„æ ·æœ¬

                for i, idx in enumerate(worst_indices):
                    memory_buffer["x"][idx] = x_batch[i]
                    memory_buffer["y"][idx] = y_batch[i]
                    memory_buffer["acc"][idx] = batch_accuracy  # æ›´æ–°å‡†ç¡®ç‡

                print(f"âœ… æ›¿æ¢ {len(x_batch)} ä¸ªä½å‡†ç¡®ç‡æ ·æœ¬ï¼Œä¿æŒè®°å¿†æ± é«˜è´¨é‡ï¼")
        else:
            print(f"âš  å½“å‰ batch å‡†ç¡®ç‡ {batch_accuracy:.2f} â‰¤ è®°å¿†æ± å¹³å‡ {memory_avg_acc:.2f}ï¼Œä¸åŠ å…¥è®°å¿†æ± ")

    def train(self):
        """
        Training process
        :return:
        """
        args = self.args

        step = 0
        batch_count = 0
        batch_acc_pos = 0
        best_acc_pos = 0.0
        step_start = time.time()

        for epoch in range(self.start_epoch, args.max_epoch):

            logging.info('-'*5 + 'Epoch {}/{}'.format(epoch, args.max_epoch - 1) + '-'*5)
            # Update the learning rate
            if self.lr_scheduler is not None:
                # self.lr_scheduler.step(epoch)
                logging.info('current lr: {}'.format(self.lr_scheduler.get_lr()))
            else:
                logging.info('current lr: {}'.format(args.lr))

            # Each epoch has a training and val phase
            for phase in ['source_train', 'source_val', 'target_val']:
                # Define the temp variable
                epoch_start = time.time()

                epoch_loss = 0.0
                batch_loss =0
                epoch_acc_pos =0
                epoch_acc_cls =0
                #######################################
                # # æ£€æŸ¥æ•°æ®åŠ è½½å™¨
                # print(type(self.dataloaders))
                # print(self.dataloaders)

                # # æ£€æŸ¥æ•°æ®åŠ è½½å™¨ä¸­çš„æ•°æ®æ•°é‡
                # for phase, dataloader in self.dataloaders.items():
                #     print(f"Phase: {phase}, Number of samples: {len(dataloader.dataset)}")

                # # æ‰“å°ä¸€äº›æ ·æœ¬æ•°æ®
                # for phase, dataloader in self.dataloaders.items():
                #     print(f"Phase: {phase}")
                #     for i, (inputs, label_pos, label_cls) in enumerate(dataloader):
                #         print(f"Sample {i+1}: inputs={inputs}, label_pos={label_pos},label_cls={label_cls}")
                #         if i >= 3:  # æ‰“å°å‰ä¸‰ä¸ªæ ·æœ¬
                #             break
                # ###############################################
                # Set model to train mode or test mode
                if phase != 'target_val':
                    if phase=='source_train':
                       self.model.train()
                    if phase=='source_val':
                       self.model.eval()
                else:
                    if args.adabn:
                        torch.save(self.model.module.state_dict() if self.device_count > 1 else self.model.state_dict(),
                                   os.path.join(self.save_dir, 'model_temp.pth'))
                        #ä¿å­˜æœ¬è½®çš„å‚æ•°æ–‡ä»¶
                        self.model_eval.load_state_dict(torch.load(os.path.join(self.save_dir, 'model_temp.pth')))
                        self.model_eval.train()
                        self.model_eval.apply(apply_dropout)
                        with torch.set_grad_enabled(False):

                            for i in range(args.adabn_epochs):
                                if args.eval_all:
                                    for batch_idx, (inputs, _,_) in enumerate(self.dataloaders['target_val']):
                                        if batch_idx == 0:
                                            inputs_all = inputs
                                        else:
                                            inputs_all = torch.cat((inputs_all, inputs), dim=0)
                                    inputs_all = inputs_all.to(self.device)
                                    _ = self.model_eval(inputs_all)
                                else:
                                    for i in range(args.adabn_epochs):
                                        for batch_idx, (inputs, _,_) in enumerate(self.dataloaders['target_val']):
                                            inputs = inputs.to(self.device)
                                            _ = self.model_eval(inputs)
                        self.model_eval.eval()
                    else:
                        self.model.eval()

                for batch_idx, (inputs, label_pos,_) in enumerate(self.dataloaders[phase]):
                    

                    # Do the learning process, in val, we do not care about the gradient for relaxing
                    with torch.set_grad_enabled(phase == 'source_train'):
                        # forward
                        if args.adabn:  
                            if phase != 'target_val':
                                pos,__annotations__ = self.model(self.set_input(inputs).to(self.device))
                            else:
                                pos,_ = self.model_eval(inputs.to(self.device))
                        else: 
                            if  phase != 'target_val':
                                inputs_train = self.set_input(inputs)  
                                pos,_ = self.model(inputs_train.to(self.device)) # phase = 'source_train'
                                print(inputs_train.shape)
                            else:
                                pos,_ = self.model(inputs.to(self.device)) # phase = 'target_val'

                       
                        label_pos = label_pos.to(self.device)                  
                        loss_pos = self.criterion(pos , label_pos) 
                        loss = loss_pos
                        pred_pos = pos.argmax(dim=1)
                        correct_pos = torch.eq(pred_pos, label_pos).float().sum().item()
                        loss_temp = loss.item() * inputs.size(0)
                        # loss_temp_cls = loss_cls.item() * inputs.size(0)
      
                        epoch_loss += loss_temp
                        epoch_acc_pos += correct_pos
                        #åœ¨è¿™ä¸ªepochä¸­æ­£ç¡®çš„æ¬¡æ•°
                        # precision = precision_score(label_pos, pred_pos, average='weighted')
                        # recall = recall_score(label_pos, pred_pos, average='weighted')
                        # f1 = f1_score(label_pos, pred_pos, average='weighted')



                        # Calculate the training information
                        if phase == 'source_train':
                            # backward
                            self.optimizer.zero_grad()
                            loss_pos.backward()
                
                            self.optimizer.step()

                            batch_loss += loss_temp
                            batch_acc_pos += correct_pos
                            batch_count += inputs.size(0)
                           
                            # Print the training information
                            # if step % args.print_step == 0:
                            #     batch_loss = batch_loss / batch_count
                            #     batch_acc_pos = batch_acc_pos / batch_count
                            #     temp_time = time.time()
                            #     train_time = temp_time - step_start
                            #     step_start = temp_time
                            #     batch_time = train_time / args.print_step if step != 0 else train_time
                            #     sample_per_sec = 1.0*batch_count/train_time  
                            #     logging.info('Epoch: {} [{}/{}], Train Loss : {:.4f} Train Acc pos: {:.4f} ,''{:.1f} examples/sec {:.2f} sec/batch'.format(
                            #         epoch, batch_idx*len(inputs), len(self.dataloaders[phase].dataset),batch_loss, batch_acc_pos,  sample_per_sec, batch_time)
                            #         )
                            #     batch_acc_pos = 0
                            #     batch_loss =  0.0
                            #     batch_count = 0
                            step += 1


                # Print the train and val information via each epoch
                epoch_loss = epoch_loss / len(self.dataloaders[phase].dataset)
                epoch_acc_pos = epoch_acc_pos / len(self.dataloaders[phase].dataset)
                epoch_acc_cls = epoch_acc_cls / len(self.dataloaders[phase].dataset)
                logging.info('Epoch: {} {}-Loss: {:.4f} {}-Acc_pos: {:.4f}, Cost {:.1f} sec'.format(
                    epoch, phase, epoch_loss, phase, epoch_acc_pos, time.time() - epoch_start
                ))

                if phase == 'source_train':
                    self.train_dict['source_train-Loss'].append(epoch_loss)
                    self.train_dict['source_train-Acc_pos'].append(epoch_acc_pos)
               
                elif phase == 'source_val':
                    self.train_dict['source_val-Loss'].append(epoch_loss)
                    self.train_dict['source_val-Acc_pos'].append(epoch_acc_pos)
       
                elif phase == 'target_val':
                    self.train_dict['target_val-Loss'].append(epoch_loss)
                    self.train_dict['target_val-Acc_pos'].append(epoch_acc_pos)

                # save the model
                if phase == 'target_val':
                    # save the checkpoint for other learning
                    model_state_dic = self.model.module.state_dict() if self.device_count > 1 else self.model.state_dict()
                    # save the best model according to the val accuracy
                    # if epoch_acc_pos > best_acc_pos   or epoch > args.max_epoch-2:
                    if epoch_acc_pos > best_acc_pos:
                        best_acc_pos = epoch_acc_pos

                        logging.info("save best model epoch {}, acc_pos {:.4f}".format(epoch, epoch_acc_pos))
                        torch.save(model_state_dic,os.path.join(self.save_dir, '{}-{:.4f}-best_model.pth'.format(epoch, best_acc_pos)))

                         # å¯¹ç›®æ ‡åŸŸæ‰€æœ‰çš„æ•°æ®è¿›è¡Œæµ‹è¯•
                        if args.eval_test_all:
                            test_start = time.time()                            
                            torch.save(model_state_dic,os.path.join(self.save_dir, 'model_test.pth'))
                            self.model_test.load_state_dict(torch.load(os.path.join(self.save_dir, 'model_test.pth')))
                            self.model_test.eval()
                            correct_pos=0
                            inputs_all = label_pos =[]
                            acc_pos = 0
                            count = 0 
                            
                            # å­˜å‚¨æ‰€æœ‰çœŸå®æ ‡ç­¾å’Œé¢„æµ‹æ ‡ç­¾ç”¨æ¥è®¡ç®—ç²¾ç¡®åº¦ã€å¬å›ç‡ã€F1
                            all_label_pos = []
                            all_pred_pos = []   

                            # å°†æ•°æ®ä¼ é€’ç»™æ¨¡å‹è¿›è¡Œæµ‹è¯•,å‡è®¾ä½ çš„æ¨¡å‹æ˜¯ model,å¯¹æ•°æ®è¿›è¡Œé¢„æµ‹
                            for batch_idx, (inputs, label_pos,label_cls) in enumerate(self.dataloaders['target_val']):
                                inputs_all = inputs
                                label_pos = label_pos
                                inputs_all = inputs_all.to(self.device)
                                label_pos = label_pos.to(self.device)    
                                self.model_test.to(self.device)
                                pos_test,_ = self.model_test(inputs_all)
                                pred_pos = pos_test.argmax(dim=1)
                                
                                correct_pos = torch.eq(pred_pos, label_pos).float().sum().item()
                                acc_pos += correct_pos
                                count += inputs.size(0)
                                
                                # æ”¶é›†å½“å‰ batch çš„é¢„æµ‹å’ŒçœŸå®æ ‡ç­¾
                                all_label_pos.extend(label_pos.cpu().numpy())
                                all_pred_pos.extend(pred_pos.cpu().numpy())

                            acc_pos = acc_pos / count 


                            logging.info('epoch: {} , acc_pos : {:.4f}, Cost {:.1f} sec'.format(
                                epoch, acc_pos , time.time() - test_start)
                                ) 
                            # è½¬æ¢ä¸º numpy æ•°ç»„
                            all_label_pos = np.array(all_label_pos)
                            all_pred_pos = np.array(all_pred_pos)

                            # è®¡ç®—ç²¾ç¡®ç‡ï¼ˆPrecisionï¼‰
                            precision_macro = precision_score(all_label_pos, all_pred_pos, average='macro')  # å®å¹³å‡
                            precision_micro = precision_score(all_label_pos, all_pred_pos, average='micro')  # å¾®å¹³å‡
                            precision_weighted = precision_score(all_label_pos, all_pred_pos, average='weighted')  # åŠ æƒå¹³å‡

                            # è®¡ç®—å¬å›ç‡ï¼ˆRecallï¼‰
                            recall_macro = recall_score(all_label_pos, all_pred_pos, average='macro')
                            recall_micro = recall_score(all_label_pos, all_pred_pos, average='micro')
                            recall_weighted = recall_score(all_label_pos, all_pred_pos, average='weighted')

                            # è®¡ç®— F1 å€¼
                            f1_macro = f1_score(all_label_pos, all_pred_pos, average='macro')
                            f1_micro = f1_score(all_label_pos, all_pred_pos, average='micro')
                            f1_weighted = f1_score(all_label_pos, all_pred_pos, average='weighted')

                            # æ‰“å°æ—¥å¿—
                            logging.info('epoch: {} , Precision (Macro): {:.4f}, Recall (Macro): {:.4f}, F1 (Macro): {:.4f}'.format(
                                epoch, precision_macro, recall_macro, f1_macro))
                            logging.info('epoch: {} ,Precision (Micro): {:.4f}, Recall (Micro): {:.4f}, F1 (Micro): {:.4f}'.format(
                                epoch, precision_micro, recall_micro, f1_micro))
                            logging.info('epoch: {} ,Precision (Weighted): {:.4f}, Recall (Weighted): {:.4f}, F1 (Weighted): {:.4f}'.format(
                                epoch, precision_weighted, recall_weighted, f1_weighted)) 
                            
                            classification_report_str = classification_report(all_label_pos, all_pred_pos)

                            # è®°å½•åˆ†ç±»æŠ¥å‘Š
                            logging.info("\nClassification Report:\n%s", classification_report_str)

                
                        
            if self.lr_scheduler is not None:
                self.lr_scheduler.step()
        


       







